{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55327369-86b1-4a21-81e8-273733eddf1e",
   "metadata": {},
   "source": [
    "# Neural Network Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75427ab-d219-4d0c-9041-9217ead02a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15fcaef-ec08-4801-88d8-12d257efc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\CBS\\Python_ML_2023\\Exercises\\Final Project\\df_wine_clean.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57aea815-b8e1-46a3-8f78-04921f02a4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'description', 'designation', 'points', 'price', 'province',\n",
       "       'region_1', 'taster_name', 'title', 'variety', 'winery',\n",
       "       'taster_weight_cat', 'clean_text', 'extracted_words',\n",
       "       'extracted_digits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80210b8-57ff-489b-9d1c-b9ff6906b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['description', 'designation',   \n",
    "       'region_1',  'title', 'variety', 'winery',\n",
    "       'taster_weight_cat', 'clean_text', 'extracted_words',\n",
    "       'extracted_digits'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e5e394-ecce-429e-9305-c35951f36179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your original DataFrame\n",
    "sample_size = int(len(df) * 1)  # 100% of the data\n",
    "sample_df = df1.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2537a91c-a6f7-45b9-a634-ebaeba9b40c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'points', 'price', 'province', 'taster_name'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb7debd-cdc9-401a-9a22-e857608a8aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of unique values in country: 43\n",
      "Sum of unique values in points: 21\n",
      "Sum of unique values in price: 542\n",
      "Sum of unique values in province: 423\n",
      "Sum of unique values in taster_name: 20\n"
     ]
    }
   ],
   "source": [
    "for column in sample_df.columns:\n",
    "    unique_values_sum = sample_df[column].nunique()\n",
    "    print(f\"Sum of unique values in {column}: {unique_values_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42fa7ca-4b15-49f9-ae49-b4db3dc1b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Create column transformer (this will help us normalize/preprocess our data)\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), ['price']), # get all values between 0 and 1\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), ['country','province', 'taster_name'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6a330f-8015-4099-9e5d-28a2ee70c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & y\n",
    "X = sample_df.drop(['points'], axis=1)\n",
    "y = sample_df['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05f114a-4ea0-486a-b640-f3e0fcfb64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build our train and test sets (use random state to ensure same split as before)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit column transformer on the training data only (doing so on test data would result in data leakage)\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "877a207f-8dee-471d-bc28-e100652a8e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x475 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a96bfb05-7199-40ed-8d39-0729024ce183",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = X_train_normal.toarray()\n",
    "X_test_dense=X_test_normal.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "051e643f-05b3-4c81-9c15-96bce3a2c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the model (3 layers, 100, 10, 1 units)\n",
    "wine_model_1 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(100),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "wine_model_1.compile(loss=tf.keras.losses.mse,\n",
    "                          optimizer=tf.keras.optimizers.Adam(),\n",
    "                          metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5e3eee50-325d-411a-ae09-7106a9b47fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3250/3250 [==============================] - 3s 827us/step - loss: 217.2125 - mse: 217.2125\n",
      "Epoch 2/200\n",
      "3250/3250 [==============================] - 3s 826us/step - loss: 7.4024 - mse: 7.4024\n",
      "Epoch 3/200\n",
      "3250/3250 [==============================] - 3s 825us/step - loss: 7.4167 - mse: 7.4167\n",
      "Epoch 4/200\n",
      "3250/3250 [==============================] - 3s 842us/step - loss: 7.4032 - mse: 7.4032\n",
      "Epoch 5/200\n",
      "3250/3250 [==============================] - 3s 865us/step - loss: 7.3862 - mse: 7.3862\n",
      "Epoch 6/200\n",
      "3250/3250 [==============================] - 3s 832us/step - loss: 7.3555 - mse: 7.3555\n",
      "Epoch 7/200\n",
      "3250/3250 [==============================] - 3s 845us/step - loss: 7.3578 - mse: 7.3578\n",
      "Epoch 8/200\n",
      "3250/3250 [==============================] - 3s 839us/step - loss: 7.3382 - mse: 7.3382\n",
      "Epoch 9/200\n",
      "3250/3250 [==============================] - 3s 840us/step - loss: 7.2974 - mse: 7.2974\n",
      "Epoch 10/200\n",
      "3250/3250 [==============================] - 3s 838us/step - loss: 7.3306 - mse: 7.3306\n",
      "Epoch 11/200\n",
      "3250/3250 [==============================] - 3s 885us/step - loss: 7.2651 - mse: 7.2651\n",
      "Epoch 12/200\n",
      "3250/3250 [==============================] - 3s 842us/step - loss: 7.2779 - mse: 7.2779\n",
      "Epoch 13/200\n",
      "3250/3250 [==============================] - 3s 841us/step - loss: 7.2823 - mse: 7.2823\n",
      "Epoch 14/200\n",
      "3250/3250 [==============================] - 3s 840us/step - loss: 7.2730 - mse: 7.2730\n",
      "Epoch 15/200\n",
      "3250/3250 [==============================] - 3s 843us/step - loss: 7.2621 - mse: 7.2621\n",
      "Epoch 16/200\n",
      "3250/3250 [==============================] - 3s 841us/step - loss: 7.2693 - mse: 7.2693\n",
      "Epoch 17/200\n",
      "3250/3250 [==============================] - 3s 881us/step - loss: 7.2333 - mse: 7.2333\n",
      "Epoch 18/200\n",
      "3250/3250 [==============================] - 3s 850us/step - loss: 7.2304 - mse: 7.2304\n",
      "Epoch 19/200\n",
      "3250/3250 [==============================] - 3s 855us/step - loss: 7.2226 - mse: 7.2226\n",
      "Epoch 20/200\n",
      "3250/3250 [==============================] - 3s 842us/step - loss: 7.2094 - mse: 7.2094\n",
      "Epoch 21/200\n",
      "3250/3250 [==============================] - 3s 843us/step - loss: 7.1917 - mse: 7.1917\n",
      "Epoch 22/200\n",
      "3250/3250 [==============================] - 3s 877us/step - loss: 7.2038 - mse: 7.2038\n",
      "Epoch 23/200\n",
      "3250/3250 [==============================] - 3s 862us/step - loss: 7.2030 - mse: 7.2030\n",
      "Epoch 24/200\n",
      "3250/3250 [==============================] - 3s 831us/step - loss: 7.2179 - mse: 7.2179\n",
      "Epoch 25/200\n",
      "3250/3250 [==============================] - 3s 828us/step - loss: 7.1929 - mse: 7.1929\n",
      "Epoch 26/200\n",
      "3250/3250 [==============================] - 3s 821us/step - loss: 7.1935 - mse: 7.1935\n",
      "Epoch 27/200\n",
      "3250/3250 [==============================] - 3s 811us/step - loss: 7.1913 - mse: 7.1913\n",
      "Epoch 28/200\n",
      "3250/3250 [==============================] - 3s 838us/step - loss: 7.1745 - mse: 7.1745\n",
      "Epoch 29/200\n",
      "3250/3250 [==============================] - 3s 822us/step - loss: 7.1627 - mse: 7.1627\n",
      "Epoch 30/200\n",
      "3250/3250 [==============================] - 3s 820us/step - loss: 7.1617 - mse: 7.1617\n",
      "Epoch 31/200\n",
      "3250/3250 [==============================] - 3s 808us/step - loss: 7.1654 - mse: 7.1654\n",
      "Epoch 32/200\n",
      "3250/3250 [==============================] - 3s 810us/step - loss: 7.1720 - mse: 7.1720\n",
      "Epoch 33/200\n",
      "3250/3250 [==============================] - 3s 825us/step - loss: 7.1778 - mse: 7.1778\n",
      "Epoch 34/200\n",
      "3250/3250 [==============================] - 3s 864us/step - loss: 7.1684 - mse: 7.1684\n",
      "Epoch 35/200\n",
      "3250/3250 [==============================] - 3s 814us/step - loss: 7.1576 - mse: 7.1576\n",
      "Epoch 36/200\n",
      "3250/3250 [==============================] - 3s 811us/step - loss: 7.1705 - mse: 7.1705\n",
      "Epoch 37/200\n",
      "3250/3250 [==============================] - 3s 810us/step - loss: 7.1500 - mse: 7.1500\n",
      "Epoch 38/200\n",
      "3250/3250 [==============================] - 3s 836us/step - loss: 7.1432 - mse: 7.1432\n",
      "Epoch 39/200\n",
      "3250/3250 [==============================] - 3s 831us/step - loss: 7.1234 - mse: 7.1234\n",
      "Epoch 40/200\n",
      "3250/3250 [==============================] - 3s 893us/step - loss: 7.1389 - mse: 7.1389\n",
      "Epoch 41/200\n",
      "3250/3250 [==============================] - 3s 837us/step - loss: 7.1406 - mse: 7.1406\n",
      "Epoch 42/200\n",
      "3250/3250 [==============================] - 3s 825us/step - loss: 7.1060 - mse: 7.1060\n",
      "Epoch 43/200\n",
      "3250/3250 [==============================] - 3s 857us/step - loss: 7.1396 - mse: 7.1396\n",
      "Epoch 44/200\n",
      "3250/3250 [==============================] - 3s 833us/step - loss: 7.1248 - mse: 7.1248\n",
      "Epoch 45/200\n",
      "3250/3250 [==============================] - 3s 836us/step - loss: 7.1145 - mse: 7.1145\n",
      "Epoch 46/200\n",
      "3250/3250 [==============================] - 3s 895us/step - loss: 7.1103 - mse: 7.1103\n",
      "Epoch 47/200\n",
      "3250/3250 [==============================] - 3s 830us/step - loss: 7.1312 - mse: 7.1312\n",
      "Epoch 48/200\n",
      "3250/3250 [==============================] - 3s 818us/step - loss: 7.1175 - mse: 7.1175\n",
      "Epoch 49/200\n",
      "3250/3250 [==============================] - 3s 827us/step - loss: 7.1129 - mse: 7.1129\n",
      "Epoch 50/200\n",
      "3250/3250 [==============================] - 3s 849us/step - loss: 7.0854 - mse: 7.0854\n",
      "Epoch 51/200\n",
      "3250/3250 [==============================] - 3s 830us/step - loss: 7.1231 - mse: 7.1231\n",
      "Epoch 52/200\n",
      "3250/3250 [==============================] - 3s 878us/step - loss: 7.1161 - mse: 7.1161\n",
      "Epoch 53/200\n",
      "3250/3250 [==============================] - 3s 836us/step - loss: 7.0882 - mse: 7.0882\n",
      "Epoch 54/200\n",
      "3250/3250 [==============================] - 3s 849us/step - loss: 7.1001 - mse: 7.1001\n",
      "Epoch 55/200\n",
      "3250/3250 [==============================] - 3s 843us/step - loss: 7.1069 - mse: 7.1069\n",
      "Epoch 56/200\n",
      "3250/3250 [==============================] - 3s 831us/step - loss: 7.0929 - mse: 7.0929\n",
      "Epoch 57/200\n",
      "3250/3250 [==============================] - 3s 842us/step - loss: 7.0797 - mse: 7.0797\n",
      "Epoch 58/200\n",
      "3250/3250 [==============================] - 3s 885us/step - loss: 7.1023 - mse: 7.1023\n",
      "Epoch 59/200\n",
      "3250/3250 [==============================] - 3s 841us/step - loss: 7.1082 - mse: 7.1082\n",
      "Epoch 60/200\n",
      "3250/3250 [==============================] - 3s 830us/step - loss: 7.0936 - mse: 7.0936\n",
      "Epoch 61/200\n",
      "3250/3250 [==============================] - 3s 830us/step - loss: 7.0989 - mse: 7.0989\n",
      "Epoch 62/200\n",
      "3250/3250 [==============================] - 3s 840us/step - loss: 7.0814 - mse: 7.0814\n",
      "Epoch 63/200\n",
      "3250/3250 [==============================] - 3s 849us/step - loss: 7.0886 - mse: 7.0886\n",
      "Epoch 64/200\n",
      "3250/3250 [==============================] - 3s 872us/step - loss: 7.0800 - mse: 7.0800\n",
      "Epoch 65/200\n",
      "3250/3250 [==============================] - 3s 841us/step - loss: 7.0868 - mse: 7.0868\n",
      "Epoch 66/200\n",
      "3250/3250 [==============================] - 3s 865us/step - loss: 7.0948 - mse: 7.0948\n",
      "Epoch 67/200\n",
      "3250/3250 [==============================] - 3s 842us/step - loss: 7.0842 - mse: 7.0842\n",
      "Epoch 68/200\n",
      "3250/3250 [==============================] - 3s 856us/step - loss: 7.0675 - mse: 7.0675\n",
      "Epoch 69/200\n",
      "3250/3250 [==============================] - 3s 897us/step - loss: 7.0686 - mse: 7.0686\n",
      "Epoch 70/200\n",
      "3250/3250 [==============================] - 3s 890us/step - loss: 7.0648 - mse: 7.0648\n",
      "Epoch 71/200\n",
      "3250/3250 [==============================] - 3s 842us/step - loss: 7.0806 - mse: 7.0806\n",
      "Epoch 72/200\n",
      "3250/3250 [==============================] - 3s 845us/step - loss: 7.0796 - mse: 7.0796\n",
      "Epoch 73/200\n",
      "3250/3250 [==============================] - 3s 857us/step - loss: 7.0790 - mse: 7.0790\n",
      "Epoch 74/200\n",
      "3250/3250 [==============================] - 3s 864us/step - loss: 7.0657 - mse: 7.0657\n",
      "Epoch 75/200\n",
      "3250/3250 [==============================] - 3s 906us/step - loss: 7.0600 - mse: 7.0600\n",
      "Epoch 76/200\n",
      "3250/3250 [==============================] - 3s 847us/step - loss: 7.0826 - mse: 7.0826\n",
      "Epoch 77/200\n",
      "3250/3250 [==============================] - 3s 857us/step - loss: 7.0788 - mse: 7.0788\n",
      "Epoch 78/200\n",
      "3250/3250 [==============================] - 3s 880us/step - loss: 7.0696 - mse: 7.0696\n",
      "Epoch 79/200\n",
      "3250/3250 [==============================] - 3s 841us/step - loss: 7.0639 - mse: 7.0639\n",
      "Epoch 80/200\n",
      "3250/3250 [==============================] - 3s 849us/step - loss: 7.0574 - mse: 7.0574\n",
      "Epoch 81/200\n",
      "3250/3250 [==============================] - 3s 897us/step - loss: 7.0714 - mse: 7.0714\n",
      "Epoch 82/200\n",
      "3250/3250 [==============================] - 3s 863us/step - loss: 7.0607 - mse: 7.0607\n",
      "Epoch 83/200\n",
      "3250/3250 [==============================] - 3s 846us/step - loss: 7.0615 - mse: 7.0615\n",
      "Epoch 84/200\n",
      "3250/3250 [==============================] - 3s 847us/step - loss: 7.0714 - mse: 7.0714\n",
      "Epoch 85/200\n",
      "3250/3250 [==============================] - 3s 846us/step - loss: 7.0509 - mse: 7.0509\n",
      "Epoch 86/200\n",
      "3250/3250 [==============================] - 3s 882us/step - loss: 7.0692 - mse: 7.0692\n",
      "Epoch 87/200\n",
      "3250/3250 [==============================] - 3s 872us/step - loss: 7.0633 - mse: 7.0633\n",
      "Epoch 88/200\n",
      "3250/3250 [==============================] - 3s 844us/step - loss: 7.0540 - mse: 7.0540\n",
      "Epoch 89/200\n",
      "3250/3250 [==============================] - 3s 850us/step - loss: 7.0492 - mse: 7.0492\n",
      "Epoch 90/200\n",
      "3250/3250 [==============================] - 3s 867us/step - loss: 7.0534 - mse: 7.0534\n",
      "Epoch 91/200\n",
      "3250/3250 [==============================] - 3s 848us/step - loss: 7.0627 - mse: 7.0627\n",
      "Epoch 92/200\n",
      "3250/3250 [==============================] - 3s 910us/step - loss: 7.0466 - mse: 7.0466\n",
      "Epoch 93/200\n",
      "3250/3250 [==============================] - 3s 863us/step - loss: 7.0425 - mse: 7.0425\n",
      "Epoch 94/200\n",
      "3250/3250 [==============================] - 3s 846us/step - loss: 7.0435 - mse: 7.0435\n",
      "Epoch 95/200\n",
      "3250/3250 [==============================] - 3s 850us/step - loss: 7.0406 - mse: 7.0406\n",
      "Epoch 96/200\n",
      "3250/3250 [==============================] - 3s 850us/step - loss: 7.0688 - mse: 7.0688\n",
      "Epoch 97/200\n",
      "3250/3250 [==============================] - 3s 848us/step - loss: 7.0454 - mse: 7.0454\n",
      "Epoch 98/200\n",
      "3250/3250 [==============================] - 3s 909us/step - loss: 7.0380 - mse: 7.0380\n",
      "Epoch 99/200\n",
      "3250/3250 [==============================] - 3s 848us/step - loss: 7.0324 - mse: 7.0324\n",
      "Epoch 100/200\n",
      "3250/3250 [==============================] - 3s 852us/step - loss: 7.0709 - mse: 7.0709\n",
      "Epoch 101/200\n",
      "3250/3250 [==============================] - 3s 851us/step - loss: 7.0419 - mse: 7.0419\n",
      "Epoch 102/200\n",
      "3250/3250 [==============================] - 3s 853us/step - loss: 7.0337 - mse: 7.0337\n",
      "Epoch 103/200\n",
      "3250/3250 [==============================] - 3s 885us/step - loss: 7.0454 - mse: 7.0454\n",
      "Epoch 104/200\n",
      "3250/3250 [==============================] - 3s 895us/step - loss: 7.0597 - mse: 7.0597\n",
      "Epoch 105/200\n",
      "3250/3250 [==============================] - 3s 851us/step - loss: 7.0396 - mse: 7.0396\n",
      "Epoch 106/200\n",
      "3250/3250 [==============================] - 3s 855us/step - loss: 7.0429 - mse: 7.0429\n",
      "Epoch 107/200\n",
      "3250/3250 [==============================] - 3s 857us/step - loss: 7.0425 - mse: 7.0425\n",
      "Epoch 108/200\n",
      "3250/3250 [==============================] - 3s 873us/step - loss: 7.0486 - mse: 7.0486\n",
      "Epoch 109/200\n",
      "3250/3250 [==============================] - 3s 918us/step - loss: 7.0401 - mse: 7.0401\n",
      "Epoch 110/200\n",
      "3250/3250 [==============================] - 3s 856us/step - loss: 7.0369 - mse: 7.0369\n",
      "Epoch 111/200\n",
      "3250/3250 [==============================] - 3s 855us/step - loss: 7.0460 - mse: 7.0460\n",
      "Epoch 112/200\n",
      "3250/3250 [==============================] - 3s 853us/step - loss: 7.0287 - mse: 7.0287\n",
      "Epoch 113/200\n",
      "3250/3250 [==============================] - 3s 858us/step - loss: 7.0498 - mse: 7.0498\n",
      "Epoch 114/200\n",
      "3250/3250 [==============================] - 3s 856us/step - loss: 7.0348 - mse: 7.0348\n",
      "Epoch 115/200\n",
      "3250/3250 [==============================] - 3s 917us/step - loss: 7.0279 - mse: 7.0279\n",
      "Epoch 116/200\n",
      "3250/3250 [==============================] - 3s 859us/step - loss: 7.0313 - mse: 7.0313\n",
      "Epoch 117/200\n",
      "3250/3250 [==============================] - 3s 870us/step - loss: 7.0413 - mse: 7.0413\n",
      "Epoch 118/200\n",
      "3250/3250 [==============================] - 3s 874us/step - loss: 7.0465 - mse: 7.0465\n",
      "Epoch 119/200\n",
      "3250/3250 [==============================] - 3s 856us/step - loss: 7.0304 - mse: 7.0304\n",
      "Epoch 120/200\n",
      "3250/3250 [==============================] - 3s 880us/step - loss: 7.0403 - mse: 7.0403\n",
      "Epoch 121/200\n",
      "3250/3250 [==============================] - 3s 895us/step - loss: 7.0155 - mse: 7.0155\n",
      "Epoch 122/200\n",
      "3250/3250 [==============================] - 3s 859us/step - loss: 7.0320 - mse: 7.0320\n",
      "Epoch 123/200\n",
      "3250/3250 [==============================] - 3s 859us/step - loss: 7.0290 - mse: 7.0290\n",
      "Epoch 124/200\n",
      "3250/3250 [==============================] - 3s 858us/step - loss: 7.0141 - mse: 7.0141\n",
      "Epoch 125/200\n",
      "3250/3250 [==============================] - 3s 858us/step - loss: 7.0280 - mse: 7.0280\n",
      "Epoch 126/200\n",
      "3250/3250 [==============================] - 3s 923us/step - loss: 7.0286 - mse: 7.0286\n",
      "Epoch 127/200\n",
      "3250/3250 [==============================] - 3s 859us/step - loss: 7.0427 - mse: 7.0427\n",
      "Epoch 128/200\n",
      "3250/3250 [==============================] - 3s 872us/step - loss: 7.0182 - mse: 7.0182\n",
      "Epoch 129/200\n",
      "3250/3250 [==============================] - 3s 858us/step - loss: 7.0235 - mse: 7.0235\n",
      "Epoch 130/200\n",
      "3250/3250 [==============================] - 3s 861us/step - loss: 7.0168 - mse: 7.0168\n",
      "Epoch 131/200\n",
      "3250/3250 [==============================] - 3s 866us/step - loss: 7.0199 - mse: 7.0199\n",
      "Epoch 132/200\n",
      "3250/3250 [==============================] - 3s 929us/step - loss: 7.0203 - mse: 7.0203\n",
      "Epoch 133/200\n",
      "3250/3250 [==============================] - 3s 872us/step - loss: 7.0123 - mse: 7.0123\n",
      "Epoch 134/200\n",
      "3250/3250 [==============================] - 3s 882us/step - loss: 7.0263 - mse: 7.0263\n",
      "Epoch 135/200\n",
      "3250/3250 [==============================] - 3s 866us/step - loss: 7.0317 - mse: 7.0317\n",
      "Epoch 136/200\n",
      "3250/3250 [==============================] - 3s 945us/step - loss: 6.9943 - mse: 6.9943\n",
      "Epoch 137/200\n",
      "3250/3250 [==============================] - 3s 994us/step - loss: 7.0220 - mse: 7.0220\n",
      "Epoch 138/200\n",
      "3250/3250 [==============================] - 3s 931us/step - loss: 7.0098 - mse: 7.0098\n",
      "Epoch 139/200\n",
      "3250/3250 [==============================] - 3s 923us/step - loss: 7.0251 - mse: 7.0251\n",
      "Epoch 140/200\n",
      "3250/3250 [==============================] - 3s 914us/step - loss: 7.0100 - mse: 7.0100\n",
      "Epoch 141/200\n",
      "3250/3250 [==============================] - 3s 911us/step - loss: 7.0100 - mse: 7.0100\n",
      "Epoch 142/200\n",
      "3250/3250 [==============================] - 3s 981us/step - loss: 7.0121 - mse: 7.0121\n",
      "Epoch 143/200\n",
      "3250/3250 [==============================] - 3s 984us/step - loss: 7.0151 - mse: 7.0151\n",
      "Epoch 144/200\n",
      "3250/3250 [==============================] - 3s 914us/step - loss: 7.0127 - mse: 7.0127\n",
      "Epoch 145/200\n",
      "3250/3250 [==============================] - 3s 943us/step - loss: 7.0144 - mse: 7.0144\n",
      "Epoch 146/200\n",
      "3250/3250 [==============================] - 3s 946us/step - loss: 7.0098 - mse: 7.0098\n",
      "Epoch 147/200\n",
      "3250/3250 [==============================] - 3s 981us/step - loss: 7.0129 - mse: 7.0129\n",
      "Epoch 148/200\n",
      "3250/3250 [==============================] - 3s 1ms/step - loss: 7.0096 - mse: 7.0096\n",
      "Epoch 149/200\n",
      "3250/3250 [==============================] - 3s 928us/step - loss: 7.0199 - mse: 7.0199\n",
      "Epoch 150/200\n",
      "3250/3250 [==============================] - 3s 930us/step - loss: 6.9918 - mse: 6.9918\n",
      "Epoch 151/200\n",
      "3250/3250 [==============================] - 3s 979us/step - loss: 7.0129 - mse: 7.0129\n",
      "Epoch 152/200\n",
      "3250/3250 [==============================] - 3s 909us/step - loss: 7.0081 - mse: 7.0081\n",
      "Epoch 153/200\n",
      "3250/3250 [==============================] - 3s 966us/step - loss: 6.9831 - mse: 6.9831\n",
      "Epoch 154/200\n",
      "3250/3250 [==============================] - 3s 912us/step - loss: 7.0079 - mse: 7.0079\n",
      "Epoch 155/200\n",
      "3250/3250 [==============================] - 3s 892us/step - loss: 6.9925 - mse: 6.9925\n",
      "Epoch 156/200\n",
      "3250/3250 [==============================] - 3s 869us/step - loss: 7.0062 - mse: 7.0062\n",
      "Epoch 157/200\n",
      "3250/3250 [==============================] - 3s 885us/step - loss: 7.0044 - mse: 7.0044\n",
      "Epoch 158/200\n",
      "3250/3250 [==============================] - 3s 943us/step - loss: 7.0149 - mse: 7.0149\n",
      "Epoch 159/200\n",
      "3250/3250 [==============================] - 3s 892us/step - loss: 7.0092 - mse: 7.0092\n",
      "Epoch 160/200\n",
      "3250/3250 [==============================] - 3s 889us/step - loss: 7.0061 - mse: 7.0061\n",
      "Epoch 161/200\n",
      "3250/3250 [==============================] - 3s 873us/step - loss: 7.0076 - mse: 7.0076\n",
      "Epoch 162/200\n",
      "3250/3250 [==============================] - 3s 869us/step - loss: 7.0184 - mse: 7.0184\n",
      "Epoch 163/200\n",
      "3250/3250 [==============================] - 3s 867us/step - loss: 7.0001 - mse: 7.0001\n",
      "Epoch 164/200\n",
      "3250/3250 [==============================] - 3s 942us/step - loss: 6.9909 - mse: 6.9909\n",
      "Epoch 165/200\n",
      "3250/3250 [==============================] - 3s 883us/step - loss: 6.9980 - mse: 6.9980\n",
      "Epoch 166/200\n",
      "3250/3250 [==============================] - 3s 885us/step - loss: 7.0050 - mse: 7.0050\n",
      "Epoch 167/200\n",
      "3250/3250 [==============================] - 3s 873us/step - loss: 7.0077 - mse: 7.0077\n",
      "Epoch 168/200\n",
      "3250/3250 [==============================] - 3s 893us/step - loss: 6.9971 - mse: 6.9971\n",
      "Epoch 169/200\n",
      "3250/3250 [==============================] - 3s 945us/step - loss: 7.0010 - mse: 7.0010\n",
      "Epoch 170/200\n",
      "3250/3250 [==============================] - 3s 884us/step - loss: 6.9830 - mse: 6.9830\n",
      "Epoch 171/200\n",
      "3250/3250 [==============================] - 3s 865us/step - loss: 6.9888 - mse: 6.9888\n",
      "Epoch 172/200\n",
      "3250/3250 [==============================] - 3s 883us/step - loss: 7.0043 - mse: 7.0043\n",
      "Epoch 173/200\n",
      "3250/3250 [==============================] - 3s 890us/step - loss: 6.9946 - mse: 6.9946\n",
      "Epoch 174/200\n",
      "3250/3250 [==============================] - 3s 876us/step - loss: 6.9805 - mse: 6.9805\n",
      "Epoch 175/200\n",
      "3250/3250 [==============================] - 3s 934us/step - loss: 7.0067 - mse: 7.0067\n",
      "Epoch 176/200\n",
      "3250/3250 [==============================] - 3s 901us/step - loss: 7.0007 - mse: 7.0007\n",
      "Epoch 177/200\n",
      "3250/3250 [==============================] - 3s 888us/step - loss: 6.9777 - mse: 6.9777\n",
      "Epoch 178/200\n",
      "3250/3250 [==============================] - 3s 889us/step - loss: 6.9762 - mse: 6.9762\n",
      "Epoch 179/200\n",
      "3250/3250 [==============================] - 3s 892us/step - loss: 6.9804 - mse: 6.9804\n",
      "Epoch 180/200\n",
      "3250/3250 [==============================] - 3s 954us/step - loss: 6.9812 - mse: 6.9812\n",
      "Epoch 181/200\n",
      "3250/3250 [==============================] - 3s 877us/step - loss: 6.9757 - mse: 6.9757\n",
      "Epoch 182/200\n",
      "3250/3250 [==============================] - 3s 877us/step - loss: 6.9953 - mse: 6.9953\n",
      "Epoch 183/200\n",
      "3250/3250 [==============================] - 3s 871us/step - loss: 6.9847 - mse: 6.9847\n",
      "Epoch 184/200\n",
      "3250/3250 [==============================] - 3s 894us/step - loss: 6.9933 - mse: 6.9933\n",
      "Epoch 185/200\n",
      "3250/3250 [==============================] - 3s 876us/step - loss: 6.9797 - mse: 6.9797\n",
      "Epoch 186/200\n",
      "3250/3250 [==============================] - 3s 968us/step - loss: 6.9823 - mse: 6.9823\n",
      "Epoch 187/200\n",
      "3250/3250 [==============================] - 3s 878us/step - loss: 6.9808 - mse: 6.9808\n",
      "Epoch 188/200\n",
      "3250/3250 [==============================] - 3s 902us/step - loss: 6.9675 - mse: 6.9675\n",
      "Epoch 189/200\n",
      "3250/3250 [==============================] - 3s 884us/step - loss: 6.9750 - mse: 6.9750\n",
      "Epoch 190/200\n",
      "3250/3250 [==============================] - 3s 918us/step - loss: 6.9845 - mse: 6.9845\n",
      "Epoch 191/200\n",
      "3250/3250 [==============================] - 3s 938us/step - loss: 6.9748 - mse: 6.9748\n",
      "Epoch 192/200\n",
      "3250/3250 [==============================] - 3s 872us/step - loss: 6.9959 - mse: 6.9959\n",
      "Epoch 193/200\n",
      "3250/3250 [==============================] - 3s 887us/step - loss: 6.9714 - mse: 6.9714\n",
      "Epoch 194/200\n",
      "3250/3250 [==============================] - 3s 911us/step - loss: 6.9860 - mse: 6.9860\n",
      "Epoch 195/200\n",
      "3250/3250 [==============================] - 3s 973us/step - loss: 6.9852 - mse: 6.9852\n",
      "Epoch 196/200\n",
      "3250/3250 [==============================] - 3s 964us/step - loss: 6.9685 - mse: 6.9685\n",
      "Epoch 197/200\n",
      "3250/3250 [==============================] - 3s 971us/step - loss: 6.9825 - mse: 6.9825\n",
      "Epoch 198/200\n",
      "3250/3250 [==============================] - 3s 934us/step - loss: 6.9813 - mse: 6.9813\n",
      "Epoch 199/200\n",
      "3250/3250 [==============================] - 3s 940us/step - loss: 6.9746 - mse: 6.9746\n",
      "Epoch 200/200\n",
      "3250/3250 [==============================] - 3s 938us/step - loss: 6.9635 - mse: 6.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abd107b5e0>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fit the model for 200 epochs \n",
    "wine_model_1.fit(X_train_dense, y_train, epochs=200, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4b3b3750-c40c-4b8a-ac23-7589f509c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_45 (Dense)            (None, 100)               47600     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,621\n",
      "Trainable params: 48,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wine_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4e426e95-25ae-4728-91a1-7f61d36ab598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 1s 603us/step - loss: 7.1382 - mse: 7.1382\n"
     ]
    }
   ],
   "source": [
    "# Evaulate 3rd model\n",
    "wine_model_1_loss, wine_model_1_mse = wine_model_1.evaluate(X_test_dense, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710170e-0359-4320-bc3c-eeb40dfa2a30",
   "metadata": {},
   "source": [
    "# model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8e4b2a69-66ce-4964-8e51-9feb2ab7a83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'description', 'designation', 'points', 'price', 'province',\n",
       "       'region_1', 'taster_name', 'title', 'variety', 'winery',\n",
       "       'taster_weight_cat', 'clean_text', 'extracted_words',\n",
       "       'extracted_digits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "84bb8976-dc09-4ad6-94ca-d4a282cfeab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop([ 'description', 'designation','clean_text','title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "28476ef6-96ea-4794-95fd-a722b216048b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'points', 'price', 'province', 'region_1', 'taster_name',\n",
       "       'variety', 'winery', 'taster_weight_cat', 'extracted_words',\n",
       "       'extracted_digits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "714796ea-bec8-4450-8a98-8db8342eda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your original DataFrame\n",
    "sample_size = int(len(df2) * 0.2)  # 20% of the data\n",
    "sample_df = df2.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0cd50f2a-a96a-4479-a620-f65a09e09f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of unique values in country: 38\n",
      "Sum of unique values in points: 21\n",
      "Sum of unique values in price: 349\n",
      "Sum of unique values in province: 316\n",
      "Sum of unique values in region_1: 934\n",
      "Sum of unique values in taster_name: 20\n",
      "Sum of unique values in variety: 473\n",
      "Sum of unique values in winery: 9344\n",
      "Sum of unique values in taster_weight_cat: 4\n",
      "Sum of unique values in extracted_words: 1153\n",
      "Sum of unique values in extracted_digits: 57\n"
     ]
    }
   ],
   "source": [
    "for column in sample_df.columns:\n",
    "    unique_values_sum = sample_df[column].nunique()\n",
    "    print(f\"Sum of unique values in {column}: {unique_values_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a06527f7-30b1-4ba3-bc93-a128d8b98ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create column transformer (this will help us normalize/preprocess our data)\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), ['price']), # get all values between 0 and 1\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"),['country', 'province', 'region_1', 'taster_name',\n",
    "       'variety', 'winery', 'taster_weight_cat', 'extracted_words',\n",
    "       'extracted_digits'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "17c0c7fb-79e9-4740-ae45-f1affec01e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & y\n",
    "X = sample_df.drop(['points'], axis=1)\n",
    "y = sample_df['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ee7b8105-3787-4534-baee-69a4d5cfd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our train and test sets (use random state to ensure same split as before)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit column transformer on the training data only (doing so on test data would result in data leakage)\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "83c1aa9b-5d49-4bec-a633-03ba586ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = X_train_normal.toarray()\n",
    "X_test_dense=X_test_normal.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "bb487a76-042e-42ed-b0d2-02d3944f7507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the model (3 layers, 100, 10, 1 units)\n",
    "wine_model_1 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(100),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "wine_model_1.compile(loss=tf.keras.losses.mse,\n",
    "                          optimizer=tf.keras.optimizers.Adam(),\n",
    "                          metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f6eccaac-ba57-4ecb-a3f5-a8c10fd718c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "650/650 [==============================] - 10s 15ms/step - loss: 775.6989 - mse: 775.6989\n",
      "Epoch 2/5\n",
      "650/650 [==============================] - 10s 15ms/step - loss: 6.6277 - mse: 6.6277\n",
      "Epoch 3/5\n",
      "650/650 [==============================] - 10s 15ms/step - loss: 4.6861 - mse: 4.6861\n",
      "Epoch 4/5\n",
      "650/650 [==============================] - 10s 15ms/step - loss: 4.3943 - mse: 4.3943\n",
      "Epoch 5/5\n",
      "650/650 [==============================] - 9s 15ms/step - loss: 4.5045 - mse: 4.5045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abceef0850>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for 5 epochs \n",
    "wine_model_1.fit(X_train_dense, y_train, epochs=5, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "19c2e36c-ad19-4532-8e23-d5b8b87143c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abcd99af40>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for 50 epochs \n",
    "wine_model_1.fit(X_train_dense, y_train, epochs=50, verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fad6cb7f-a106-4ac9-baa2-32c46a3e7ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 100)               1125500   \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,126,521\n",
      "Trainable params: 1,126,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wine_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "02dfcf0b-cab4-4d5b-9619-7df8f777af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 0s 1ms/step - loss: 7.6817 - mse: 7.6817\n"
     ]
    }
   ],
   "source": [
    "# Evaulate 2rd model\n",
    "wine_model_1_loss, wine_model_1_mse = wine_model_1.evaluate(X_test_dense, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4190ece-156e-41eb-bea5-993db9cb7eb2",
   "metadata": {},
   "source": [
    "# model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "42528e70-a019-48f7-8624-a800f0d945b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'description', 'designation', 'points', 'price', 'province',\n",
       "       'region_1', 'taster_name', 'title', 'variety', 'winery',\n",
       "       'taster_weight_cat', 'clean_text', 'extracted_words',\n",
       "       'extracted_digits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1f602e74-a496-4d38-be2d-3195df0f6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.drop([ 'description', 'designation','clean_text','title','winery','extracted_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d4b6c315-caa3-41fa-b7dd-9d78ad2e11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your original DataFrame\n",
    "sample_size = int(len(df3) * 1)  # 100% of the data\n",
    "sample_df = df3.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7e831d6a-5d50-45a4-aabf-0887e3798c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of unique values in country: 43\n",
      "Sum of unique values in points: 21\n",
      "Sum of unique values in price: 542\n",
      "Sum of unique values in province: 423\n",
      "Sum of unique values in region_1: 1230\n",
      "Sum of unique values in taster_name: 20\n",
      "Sum of unique values in variety: 708\n",
      "Sum of unique values in taster_weight_cat: 4\n",
      "Sum of unique values in extracted_digits: 87\n"
     ]
    }
   ],
   "source": [
    "for column in sample_df.columns:\n",
    "    unique_values_sum = sample_df[column].nunique()\n",
    "    print(f\"Sum of unique values in {column}: {unique_values_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba86e7b5-36a5-41e8-bd36-b0e6fbfee041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column transformer (this will help us normalize/preprocess our data)\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), ['price']), # get all values between 0 and 1\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"),['country', 'province', 'region_1', 'taster_name',\n",
    "       'variety',  'taster_weight_cat',\n",
    "       'extracted_digits'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fafe1565-fc98-479b-8be0-ce22356d5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X & y\n",
    "X = sample_df.drop(['points'], axis=1)\n",
    "y = sample_df['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "438b6fa6-2de7-41b8-9c58-975e088b3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our train and test sets (use random state to ensure same split as before)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit column transformer on the training data only (doing so on test data would result in data leakage)\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ea417d9b-0f11-4b5b-b3b3-8a6a314f3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = X_train_normal.toarray()\n",
    "X_test_dense=X_test_normal.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ec045417-a078-4dd7-88f8-8388f2cf6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the model (3 layers, 100, 10, 1 units)\n",
    "wine_model_1 = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(100),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "wine_model_1.compile(loss=tf.keras.losses.mse,\n",
    "                          optimizer=tf.keras.optimizers.Adam(),\n",
    "                          metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "784ba4d5-2586-4500-8fdf-8f392c3f734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 156.9802 - mse: 156.9802\n",
      "Epoch 2/5\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.9048 - mse: 6.9048\n",
      "Epoch 3/5\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.8139 - mse: 6.8139\n",
      "Epoch 4/5\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.6546 - mse: 6.6546\n",
      "Epoch 5/5\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.5789 - mse: 6.5789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abd0fc8580>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for 5 epochs \n",
    "wine_model_1.fit(X_train_dense, y_train, epochs=5, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b8e9b2d3-31ff-47ed-ad40-c6c537c0a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 1s 699us/step - loss: 6.7001 - mse: 6.7001\n"
     ]
    }
   ],
   "source": [
    "# Evaulate 3rd model\n",
    "wine_model_1_loss, wine_model_1_mse = wine_model_1.evaluate(X_test_dense, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "777cb297-eb7f-492c-9671-5f699cbfdb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.4911 - mse: 6.4911\n",
      "Epoch 2/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.4699 - mse: 6.4699\n",
      "Epoch 3/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.4141 - mse: 6.4141\n",
      "Epoch 4/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.3515 - mse: 6.3515\n",
      "Epoch 5/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.3402 - mse: 6.3402\n",
      "Epoch 6/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.3031 - mse: 6.3031\n",
      "Epoch 7/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.2877 - mse: 6.2877\n",
      "Epoch 8/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.2765 - mse: 6.2765\n",
      "Epoch 9/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.2535 - mse: 6.2535\n",
      "Epoch 10/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.2625 - mse: 6.2625\n",
      "Epoch 11/50\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 6.1884 - mse: 6.1884\n",
      "Epoch 12/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.1953 - mse: 6.1953\n",
      "Epoch 13/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.1825 - mse: 6.1825\n",
      "Epoch 14/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.1774 - mse: 6.1774\n",
      "Epoch 15/50\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 6.1689 - mse: 6.1689\n",
      "Epoch 16/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.1660 - mse: 6.1660\n",
      "Epoch 17/50\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 6.1137 - mse: 6.1137\n",
      "Epoch 18/50\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 6.1458 - mse: 6.1458\n",
      "Epoch 19/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.1127 - mse: 6.1127\n",
      "Epoch 20/50\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 6.1287 - mse: 6.1287\n",
      "Epoch 21/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0724 - mse: 6.0724\n",
      "Epoch 22/50\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 6.1059 - mse: 6.1059\n",
      "Epoch 23/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0812 - mse: 6.0812\n",
      "Epoch 24/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0846 - mse: 6.0846\n",
      "Epoch 25/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0583 - mse: 6.0583\n",
      "Epoch 26/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0628 - mse: 6.0628\n",
      "Epoch 27/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0595 - mse: 6.0595\n",
      "Epoch 28/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0525 - mse: 6.0525\n",
      "Epoch 29/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0319 - mse: 6.0319\n",
      "Epoch 30/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0379 - mse: 6.0379\n",
      "Epoch 31/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0265 - mse: 6.0265\n",
      "Epoch 32/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0322 - mse: 6.0322\n",
      "Epoch 33/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0361 - mse: 6.0361\n",
      "Epoch 34/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0022 - mse: 6.0022\n",
      "Epoch 35/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0066 - mse: 6.0066\n",
      "Epoch 36/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0171 - mse: 6.0171\n",
      "Epoch 37/50\n",
      "3250/3250 [==============================] - 5s 1ms/step - loss: 6.0033 - mse: 6.0033\n",
      "Epoch 38/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 6.0190 - mse: 6.0190\n",
      "Epoch 39/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9772 - mse: 5.9772\n",
      "Epoch 40/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9916 - mse: 5.9916\n",
      "Epoch 41/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9971 - mse: 5.9971\n",
      "Epoch 42/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9613 - mse: 5.9613\n",
      "Epoch 43/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9821 - mse: 5.9821\n",
      "Epoch 44/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9557 - mse: 5.9557\n",
      "Epoch 45/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9644 - mse: 5.9644\n",
      "Epoch 46/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9682 - mse: 5.9682\n",
      "Epoch 47/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9789 - mse: 5.9789\n",
      "Epoch 48/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9712 - mse: 5.9712\n",
      "Epoch 49/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9577 - mse: 5.9577\n",
      "Epoch 50/50\n",
      "3250/3250 [==============================] - 4s 1ms/step - loss: 5.9257 - mse: 5.9257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1abd3a2f520>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for 50 epochs \n",
    "wine_model_1.fit(X_train_dense, y_train, epochs=50, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c7e18ea9-be01-4d04-811f-b0a624bb22d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 1s 761us/step - loss: 6.0244 - mse: 6.0244\n"
     ]
    }
   ],
   "source": [
    "# Evaulate 3rd model\n",
    "wine_model_1_loss, wine_model_1_mse = wine_model_1.evaluate(X_test_dense, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da649cf-1d17-4785-ba26-b43e1bf2bffc",
   "metadata": {},
   "source": [
    "### nlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f1726b-02e5-4c2b-8d07-04ed97de9b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'description', 'designation', 'points', 'price', 'province',\n",
       "       'region_1', 'taster_name', 'title', 'variety', 'winery',\n",
       "       'taster_weight_cat', 'clean_text', 'extracted_words',\n",
       "       'extracted_digits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdca7aee-710f-48b6-a689-ffe9f2cb3a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>taster_weight_cat</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>extracted_words</th>\n",
       "      <th>extracted_digits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72773</th>\n",
       "      <td>France</td>\n",
       "      <td>This Chardonnay's acidity and minerally struct...</td>\n",
       "      <td>other</td>\n",
       "      <td>88</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>Puligny-Montrachet</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Joseph Faiveley 2010  Puligny-Montrachet</td>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>Joseph Faiveley</td>\n",
       "      <td>Category B</td>\n",
       "      <td>chardonnay acid miner structur fine balanc pal...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90612</th>\n",
       "      <td>Italy</td>\n",
       "      <td>This blend of Garganega (the grape that makes ...</td>\n",
       "      <td>other</td>\n",
       "      <td>83</td>\n",
       "      <td>27.047835</td>\n",
       "      <td>Northeastern Italy</td>\n",
       "      <td>Delle Venezie</td>\n",
       "      <td>other</td>\n",
       "      <td>Duca del Frassino 2011 White (Delle Venezie)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Duca del Frassino</td>\n",
       "      <td>Category B</td>\n",
       "      <td>blend garganega grape make soav wine pinot gri...</td>\n",
       "      <td>['Delle Venezie']</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121327</th>\n",
       "      <td>France</td>\n",
       "      <td>A concentrated, rich wine, packed with peach a...</td>\n",
       "      <td>Grand'Chaille</td>\n",
       "      <td>91</td>\n",
       "      <td>25.998123</td>\n",
       "      <td>Loire Valley</td>\n",
       "      <td>Sancerre</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>Domaine Thomas &amp; Fils 2008 Grand'Chaille  (San...</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Domaine Thomas &amp; Fils</td>\n",
       "      <td>Category B</td>\n",
       "      <td>concentr rich wine pack peach pear flavor fres...</td>\n",
       "      <td>['Sancerre']</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46544</th>\n",
       "      <td>US</td>\n",
       "      <td>Lots of rich blueberry, cherry and blackberry ...</td>\n",
       "      <td>Hope Family Vineyard</td>\n",
       "      <td>87</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>California</td>\n",
       "      <td>Paso Robles</td>\n",
       "      <td>other</td>\n",
       "      <td>Austin Hope 2011 Hope Family Vineyard Syrah (P...</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>Austin Hope</td>\n",
       "      <td>Category B</td>\n",
       "      <td>lot rich blueberri cherri blackberri fruit fla...</td>\n",
       "      <td>['Paso Robles']</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>US</td>\n",
       "      <td>This is a very rich Pinot whose primary virtue...</td>\n",
       "      <td>Wiley Vineyard</td>\n",
       "      <td>88</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>California</td>\n",
       "      <td>Anderson Valley</td>\n",
       "      <td>other</td>\n",
       "      <td>Harrington 2006 Wiley Vineyard Pinot Noir (And...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Harrington</td>\n",
       "      <td>Category B</td>\n",
       "      <td>rich pinot whose primari virtu fruit explod de...</td>\n",
       "      <td>['Anderson Valley']</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                                        description  \\\n",
       "72773   France  This Chardonnay's acidity and minerally struct...   \n",
       "90612    Italy  This blend of Garganega (the grape that makes ...   \n",
       "121327  France  A concentrated, rich wine, packed with peach a...   \n",
       "46544       US  Lots of rich blueberry, cherry and blackberry ...   \n",
       "186         US  This is a very rich Pinot whose primary virtue...   \n",
       "\n",
       "                 designation  points      price            province  \\\n",
       "72773                  other      88  75.000000            Burgundy   \n",
       "90612                  other      83  27.047835  Northeastern Italy   \n",
       "121327         Grand'Chaille      91  25.998123        Loire Valley   \n",
       "46544   Hope Family Vineyard      87  42.000000          California   \n",
       "186           Wiley Vineyard      88  40.000000          California   \n",
       "\n",
       "                  region_1 taster_name  \\\n",
       "72773   Puligny-Montrachet  Roger Voss   \n",
       "90612        Delle Venezie       other   \n",
       "121327            Sancerre  Roger Voss   \n",
       "46544          Paso Robles       other   \n",
       "186        Anderson Valley       other   \n",
       "\n",
       "                                                    title          variety  \\\n",
       "72773            Joseph Faiveley 2010  Puligny-Montrachet       Chardonnay   \n",
       "90612        Duca del Frassino 2011 White (Delle Venezie)      White Blend   \n",
       "121327  Domaine Thomas & Fils 2008 Grand'Chaille  (San...  Sauvignon Blanc   \n",
       "46544   Austin Hope 2011 Hope Family Vineyard Syrah (P...            Syrah   \n",
       "186     Harrington 2006 Wiley Vineyard Pinot Noir (And...       Pinot Noir   \n",
       "\n",
       "                       winery taster_weight_cat  \\\n",
       "72773         Joseph Faiveley        Category B   \n",
       "90612       Duca del Frassino        Category B   \n",
       "121327  Domaine Thomas & Fils        Category B   \n",
       "46544             Austin Hope        Category B   \n",
       "186                Harrington        Category B   \n",
       "\n",
       "                                               clean_text  \\\n",
       "72773   chardonnay acid miner structur fine balanc pal...   \n",
       "90612   blend garganega grape make soav wine pinot gri...   \n",
       "121327  concentr rich wine pack peach pear flavor fres...   \n",
       "46544   lot rich blueberri cherri blackberri fruit fla...   \n",
       "186     rich pinot whose primari virtu fruit explod de...   \n",
       "\n",
       "            extracted_words extracted_digits  \n",
       "72773                    []             2010  \n",
       "90612     ['Delle Venezie']             2011  \n",
       "121327         ['Sancerre']             2008  \n",
       "46544       ['Paso Robles']             2011  \n",
       "186     ['Anderson Valley']             2006  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac2f5db-87b7-447d-8b9d-b635ae6fcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"description\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"points\"].to_numpy(),\n",
    "                                                                            test_size=0.2, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dd30a4f-294a-4427-8291-46af65a5e982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103971, 103971, 25993, 25993)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5797621-c6b4-4fa1-aac2-b1de101530d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a0c150a-8cf8-4159-aa2a-5996eeade6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text vectorization (tokenization)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization # after TensorFlow 2.6\n",
    "\n",
    "\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d05b0801-a2b1-4d07-be26-84abb82df0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find average number of tokens (words) in training Tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5434f41a-a3dd-4bb0-9d12-95fe6cecabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 40 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874233a6-a11d-4343-acf2-243cd8380849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ca3843c-04cc-4b17-9d9c-38b4ce4b8a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40), dtype=int64, numpy=\n",
       "array([[ 122,    4, 2370,   11, 4792, 7679,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41ff75df-a9a0-441d-8076-5255af6ef11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "From the former Ashley's Vineyard, one of the best-known in the AVA, comes this very ripe, extracted Pinot Noir. Tastes like it had a long hangtime, getting ripe to the point of baked pie-filling cherries and pured, smoky, almost raisined blackberries. The tradeoff for all this richness is softness and a resulting one-dimensionality.      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40), dtype=int64, numpy=\n",
       "array([[  22,    3, 3031,    1,  151,  157,    5,    3,    1,   11,    3,\n",
       "        2097,  240,    7,   66,   27,  690,   98,  238,  203,  109,   12,\n",
       "        1895,    4,  118, 3792, 2697,   27,   13,    3,  890,    5,  229,\n",
       "        3137,  173,    2,    1,  169,  188, 5110]], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82b9e0a8-404b-42f3-a183-c29853e35045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'and', 'the', 'a']\n",
      "Bottom 5 least common words: ['softbodied', 'soares', 'snacks', 'smidgens', 'smackingly']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e9bf0-617d-4d08-b969-d2bc1d142abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating an Embedding using an Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adb6a602-bee4-4dc4-8967-a4190a8cbc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x1f9428d4880>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") \n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3bc2c96-5d7a-432b-bec8-b1cb2a8a0ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "This 100% varietal wine offers aromas of coffee, green herbs, pencil lead and vanilla followed by silky cherry and dark chocolate flavors. There's plenty of appeal but the variety seems to get lost.      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40, 128), dtype=float32, numpy=\n",
       "array([[[ 0.04853568, -0.04433579, -0.03145378, ...,  0.00175725,\n",
       "          0.00621579,  0.04214795],\n",
       "        [-0.04442856,  0.04505548,  0.02003416, ..., -0.00263672,\n",
       "         -0.00598887, -0.0089368 ],\n",
       "        [-0.04153187,  0.01386137, -0.0465541 , ...,  0.01098907,\n",
       "         -0.01932958, -0.02483048],\n",
       "        ...,\n",
       "        [-0.01450722, -0.04854118,  0.02312665, ...,  0.02064779,\n",
       "         -0.04664967,  0.03627522],\n",
       "        [-0.01450722, -0.04854118,  0.02312665, ...,  0.02064779,\n",
       "         -0.04664967,  0.03627522],\n",
       "        [-0.01450722, -0.04854118,  0.02312665, ...,  0.02064779,\n",
       "         -0.04664967,  0.03627522]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9180c492-8714-4905-9357-1b55c6d01e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([-0.03333165, -0.04857239, -0.01054839,  0.0272988 , -0.00382303,\n",
       "       -0.0480469 ,  0.03834609, -0.02931286, -0.01784631,  0.00357044,\n",
       "        0.04381588, -0.03250489,  0.02170971,  0.03339458, -0.03612515,\n",
       "        0.03084313,  0.04319126,  0.00799993, -0.00022131,  0.02822665,\n",
       "       -0.00847627,  0.03491584, -0.03364166, -0.0380974 ,  0.02328969,\n",
       "       -0.03538864,  0.03966046, -0.02955651,  0.01785297,  0.01936351,\n",
       "       -0.04831639, -0.0473676 ,  0.04827124, -0.00525031, -0.04869051,\n",
       "        0.0066415 ,  0.02795322,  0.02386634, -0.01266098, -0.01881822,\n",
       "        0.01192392, -0.04031923, -0.0169936 ,  0.00448623,  0.03942652,\n",
       "       -0.01219716, -0.00602226, -0.02396879,  0.02290897,  0.00073887,\n",
       "        0.00834299, -0.0428276 , -0.04159749, -0.04496205,  0.03248252,\n",
       "        0.00365256,  0.04084495, -0.04093276, -0.04815407, -0.04578024,\n",
       "       -0.03116703, -0.04572845, -0.02802062,  0.02249709, -0.01061878,\n",
       "        0.03462151, -0.0187379 , -0.01290197,  0.01680208,  0.02623541,\n",
       "       -0.04512597,  0.02484646, -0.00884144,  0.03880954,  0.02724027,\n",
       "        0.0490743 , -0.03255481,  0.02836266, -0.04893173, -0.02313781,\n",
       "       -0.00981135, -0.03629155,  0.02556999, -0.04439   ,  0.00565218,\n",
       "       -0.04577682,  0.01673291,  0.02294319, -0.04875386,  0.01514496,\n",
       "       -0.0186064 , -0.02991205,  0.0091855 ,  0.04117915, -0.02732449,\n",
       "        0.00349585, -0.03354083, -0.030788  , -0.04199101, -0.0263389 ,\n",
       "        0.00787433, -0.01349096, -0.03393167, -0.01245209, -0.03611856,\n",
       "       -0.01637203,  0.02157428,  0.00754   ,  0.00372631,  0.04906103,\n",
       "       -0.04684046,  0.0342576 , -0.00925171, -0.01968607, -0.04409187,\n",
       "        0.01258044, -0.00653924,  0.02629664,  0.04054824,  0.04357952,\n",
       "       -0.0353572 , -0.0267294 ,  0.01830873, -0.0087548 , -0.0183987 ,\n",
       "       -0.03748137,  0.02562809,  0.02122611], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38cb29-2a4c-4406-89ea-88c69b75593a",
   "metadata": {},
   "source": [
    "# Modelling a text dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2b265-5706-4b12-b669-6e5ab090ee0b",
   "metadata": {},
   "source": [
    "# nlp- Model 0: Getting a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b570ed94-51cf-481b-8786-c76610401a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;regressor&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
       "                (&#x27;regressor&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('regressor', LinearRegression())])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modeling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),  # convert words to numbers using tfidf\n",
    "    (\"regressor\", LinearRegression())  # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3dd4b5d-935a-4a76-afff-844720d8b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 63.29%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8390b25c-6795-4f89-8564-1f43a1310e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([87.8161736 , 86.2279884 , 87.16783131, 92.42746795, 89.75495362,\n",
       "       89.24717649, 83.01867574, 84.06951229, 89.59291174, 90.21629532,\n",
       "       90.30972153, 95.41330441, 86.02482296, 91.58223797, 87.04616643,\n",
       "       92.09174326, 86.79244805, 89.78592128, 87.92800359, 91.83048807])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc17db2b-84c0-4f24-a45e-0ffcb18872a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model MAE, MSE, and R-squared of a regression model.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    y_true: true labels in the form of a 1D array\n",
    "    y_pred: predicted labels in the form of a 1D array\n",
    "\n",
    "    Returns a dictionary of MAE, MSE, and R-squared.\n",
    "    \"\"\"\n",
    "    # Calculate MAE\n",
    "    model_mae = mean_absolute_error(y_true, y_pred)\n",
    "    # Calculate MSE\n",
    "    model_mse = mean_squared_error(y_true, y_pred)\n",
    "    # Calculate R-squared\n",
    "    model_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    model_results = {\"MAE\": model_mae,\n",
    "                     \"MSE\": model_mse,\n",
    "                     \"R-squared\": model_r2}\n",
    "\n",
    "    return model_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36239527-6c2a-483f-a7c8-21a7d278a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 1.4076252944179053,\n",
       " 'MSE': 3.4218086321893355,\n",
       " 'R-squared': 0.6328651260319366}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6734be0-928e-470d-ae8c-b5ba3bef24b5",
   "metadata": {},
   "source": [
    "# Model 1: A simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70e19ac1-ff0a-4956-b6f5-33b07ead7ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model architecture\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")  # Inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs)  # Turn the input text into numbers\n",
    "x = embedding(x)  # Create an embedding of the numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x)  # Lower the dimensionality of the embedding\n",
    "outputs = layers.Dense(1, activation=\"linear\")(x)  # Create the output layer with linear activation\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")  # Construct the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae6b0833-dca9-4915-8bb3-acdd4f99c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for regression\n",
    "model_1.compile(loss=\"mean_squared_error\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mean_squared_error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d984f111-1e5b-46ed-b8d1-b2956a00fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 40)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 40, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "0d7fa8b5-22d2-43a7-9ccf-41c667d0a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3250/3250 [==============================] - 63s 19ms/step - loss: 3.4100 - mean_squared_error: 3.4100 - val_loss: 3.6772 - val_mean_squared_error: 3.6772\n",
      "Epoch 2/5\n",
      "3250/3250 [==============================] - 63s 19ms/step - loss: 3.1895 - mean_squared_error: 3.1895 - val_loss: 3.5743 - val_mean_squared_error: 3.5743\n",
      "Epoch 3/5\n",
      "3250/3250 [==============================] - 64s 20ms/step - loss: 3.0716 - mean_squared_error: 3.0716 - val_loss: 3.4620 - val_mean_squared_error: 3.4620\n",
      "Epoch 4/5\n",
      "3250/3250 [==============================] - 63s 20ms/step - loss: 3.0144 - mean_squared_error: 3.0144 - val_loss: 3.4239 - val_mean_squared_error: 3.4239\n",
      "Epoch 5/5\n",
      "3250/3250 [==============================] - 64s 20ms/step - loss: 2.9832 - mean_squared_error: 2.9832 - val_loss: 3.4383 - val_mean_squared_error: 3.4383\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "26c0143c-7210-40f0-8b8f-3b72451d0d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 1s 1ms/step - loss: 3.4383 - mean_squared_error: 3.4383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.4382917881011963, 3.4382917881011963]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "7a4092a4-a273-419e-bd67-3e75e0968521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 1s 946us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[88.44615 ],\n",
       "       [86.40053 ],\n",
       "       [86.7319  ],\n",
       "       [91.00393 ],\n",
       "       [90.6269  ],\n",
       "       [90.50606 ],\n",
       "       [83.442535],\n",
       "       [83.86459 ],\n",
       "       [90.184685],\n",
       "       [91.1009  ]], dtype=float32)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (these come back in the form of probabilities)\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b9203642-c5c4-4ed4-bdf4-1f236d82f746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([88., 86., 87., 91., 91., 91., 83., 84., 90., 91., 90., 97., 86.,\n",
       "       91., 88., 92., 86., 90., 88., 88.], dtype=float32)>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "64ca1242-f223-43f2-99a1-f2472dac86d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 1.4372331012195592,\n",
       " 'MSE': 3.5133305120609393,\n",
       " 'R-squared': 0.6230455021301513}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 metrics\n",
    "model_1_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9a5a3b96-ba6f-4006-b5ba-7d8374c429f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 1.41, New MAE: 1.44, Difference: 0.03\n",
      "Baseline MSE: 3.42, New MSE: 3.51, Difference: 0.09\n",
      "Baseline R-squared: 0.63, New R-squared: 0.62, Difference: -0.01\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "  for key, value in baseline_results.items():\n",
    "    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n",
    "\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11afca8-bf69-4756-8e8a-c39c2168005d",
   "metadata": {},
   "source": [
    "# Model 2: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "f76d1e5f-a6ff-41f3-a294-461e601e4458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 40, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "\n",
    "# Create LSTM model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
    "print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "d80f9dd8-6712-4e1b-afb4-0406009382be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for regression\n",
    "model_2.compile(loss=\"mean_squared_error\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mean_squared_error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1f87f599-38db-45be-9126-b00445fec02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "85319c75-09c9-4e3b-b022-81d958928232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3250/3250 [==============================] - 95s 29ms/step - loss: 884.0226 - mean_squared_error: 884.0226 - val_loss: 9.3303 - val_mean_squared_error: 9.3303\n",
      "Epoch 2/100\n",
      "3250/3250 [==============================] - 97s 30ms/step - loss: 9.2285 - mean_squared_error: 9.2285 - val_loss: 9.3255 - val_mean_squared_error: 9.3255\n",
      "Epoch 3/100\n",
      "3250/3250 [==============================] - 97s 30ms/step - loss: 8.7837 - mean_squared_error: 8.7837 - val_loss: 4.6252 - val_mean_squared_error: 4.6252\n",
      "Epoch 4/100\n",
      "3250/3250 [==============================] - 93s 29ms/step - loss: 3.6125 - mean_squared_error: 3.6125 - val_loss: 3.3056 - val_mean_squared_error: 3.3056\n",
      "Epoch 5/100\n",
      "3250/3250 [==============================] - 94s 29ms/step - loss: 2.9155 - mean_squared_error: 2.9155 - val_loss: 3.1543 - val_mean_squared_error: 3.1543\n",
      "Epoch 6/100\n",
      "3250/3250 [==============================] - 95s 29ms/step - loss: 2.6219 - mean_squared_error: 2.6219 - val_loss: 3.1082 - val_mean_squared_error: 3.1082\n",
      "Epoch 7/100\n",
      "3250/3250 [==============================] - 96s 30ms/step - loss: 2.4250 - mean_squared_error: 2.4250 - val_loss: 3.1213 - val_mean_squared_error: 3.1213\n",
      "Epoch 8/100\n",
      "3250/3250 [==============================] - 96s 30ms/step - loss: 2.2698 - mean_squared_error: 2.2698 - val_loss: 3.0765 - val_mean_squared_error: 3.0765\n",
      "Epoch 9/100\n",
      "3250/3250 [==============================] - 97s 30ms/step - loss: 2.1278 - mean_squared_error: 2.1278 - val_loss: 3.1177 - val_mean_squared_error: 3.1177\n",
      "Epoch 10/100\n",
      "3250/3250 [==============================] - 96s 30ms/step - loss: 2.0048 - mean_squared_error: 2.0048 - val_loss: 3.1398 - val_mean_squared_error: 3.1398\n",
      "Epoch 11/100\n",
      "3250/3250 [==============================] - 96s 29ms/step - loss: 1.8848 - mean_squared_error: 1.8848 - val_loss: 3.1857 - val_mean_squared_error: 3.1857\n",
      "Epoch 12/100\n",
      "3250/3250 [==============================] - 94s 29ms/step - loss: 1.7761 - mean_squared_error: 1.7761 - val_loss: 3.3477 - val_mean_squared_error: 3.3477\n",
      "Epoch 13/100\n",
      "3250/3250 [==============================] - 95s 29ms/step - loss: 1.6705 - mean_squared_error: 1.6705 - val_loss: 3.3043 - val_mean_squared_error: 3.3043\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_2_history = model_2.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=100,\n",
    "                              validation_data=(val_sentences, val_labels),  callbacks=[early_stopping],\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "30b76952-1f9a-42ce-9ff8-77f96bd77680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 4s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25993, 1),\n",
       " array([[87.91096 ],\n",
       "        [88.67603 ],\n",
       "        [86.357704],\n",
       "        [91.01722 ],\n",
       "        [89.61097 ],\n",
       "        [89.647675],\n",
       "        [82.598015],\n",
       "        [83.27592 ],\n",
       "        [91.06005 ],\n",
       "        [92.27299 ]], dtype=float32))"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "5747bd7b-7433-4912-96d0-fdb664eae42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([88., 89., 86., 91., 90., 90., 83., 83., 91., 92.], dtype=float32)>"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round out predictions and reduce to 1-dimensional array\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "99fa1cb4-f904-4422-b9e4-8fe36c0b9801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 1.3703304735890431,\n",
       " 'MSE': 3.3854884007232715,\n",
       " 'R-squared': 0.6367620194690347}"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate LSTM model results\n",
    "model_2_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "7f6a22e8-0d5c-4336-b510-38777982ff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 1.41, New MAE: 1.37, Difference: -0.04\n",
      "Baseline MSE: 3.42, New MSE: 3.39, Difference: -0.04\n",
      "Baseline R-squared: 0.63, New R-squared: 0.64, Difference: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Compare model 2 to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9bc54a-4763-468c-aa5d-57baf456da15",
   "metadata": {},
   "source": [
    "# Model 3: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "6cec57cf-8a8d-4cff-b1a5-cd2ce9938fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 40, 128]), TensorShape([1, 36, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the embedding, 1D convolutional and max pooling\n",
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sentence into embedding\n",
    "conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over target sequence 5 words at a time\n",
    "conv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer\n",
    "max_pool = layers.GlobalMaxPool1D() \n",
    "max_pool_output = max_pool(conv_1d_output) # get the most important features\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "1b67be5b-ecd0-45bc-bb15-8078a06b5bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 40, 128), dtype=float32, numpy=\n",
       " array([[[ 0.53918016, -0.6094435 ,  0.47952527, ...,  0.52343374,\n",
       "           0.52661276, -0.56679463],\n",
       "         [ 0.4566596 , -0.46478346,  0.5033103 , ...,  0.47044986,\n",
       "           0.54187506, -0.5501663 ],\n",
       "         [ 0.50307244, -0.57592446,  0.46362057, ...,  0.5673405 ,\n",
       "           0.57735085, -0.5065696 ],\n",
       "         ...,\n",
       "         [ 0.49757418, -0.48028332,  0.46292648, ...,  0.5332629 ,\n",
       "           0.47543415, -0.53812623],\n",
       "         [ 0.49757418, -0.48028332,  0.46292648, ...,  0.5332629 ,\n",
       "           0.47543415, -0.53812623],\n",
       "         [ 0.49757418, -0.48028332,  0.46292648, ...,  0.5332629 ,\n",
       "           0.47543415, -0.53812623]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 36, 32), dtype=float32, numpy=\n",
       " array([[[0.        , 0.512009  , 1.153581  , ..., 0.        ,\n",
       "          0.49206328, 0.        ],\n",
       "         [0.        , 0.5182718 , 1.2225544 , ..., 0.        ,\n",
       "          0.59311503, 0.        ],\n",
       "         [0.        , 0.5559645 , 1.2406293 , ..., 0.        ,\n",
       "          0.47756255, 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.54857606, 1.0912074 , ..., 0.        ,\n",
       "          0.42914104, 0.        ],\n",
       "         [0.        , 0.54857606, 1.0912074 , ..., 0.        ,\n",
       "          0.42914104, 0.        ],\n",
       "         [0.        , 0.54857606, 1.0912074 , ..., 0.        ,\n",
       "          0.42914104, 0.        ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       " array([[0.        , 0.5559645 , 1.2406293 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 1.4394801 , 0.7890542 ,\n",
       "         0.25033343, 0.        , 0.        , 0.15079117, 0.        ,\n",
       "         0.        , 0.        , 0.47595766, 0.        , 0.        ,\n",
       "         1.4711415 , 0.17650053, 0.        , 0.14078274, 0.        ,\n",
       "         0.81253475, 0.        , 0.3690828 , 0.31617916, 0.        ,\n",
       "         0.59311503, 0.        ]], dtype=float32)>)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the outputs of each layer\n",
    "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d2ff2243-6101-4d01-a3c5-9f10d2810178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 40)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 40, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 36, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,300,545\n",
      "Trainable params: 1,300,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_5\")\n",
    "\n",
    "# Create 1-dimensional convolutional layer to model sequences\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_3_embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\n",
    "outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "# Compile Conv1D model\n",
    "model_3.compile(loss=\"mean_squared_error\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mse\"])\n",
    "\n",
    "# Get a summary of our 1D convolution model\n",
    "model_3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "c0d57d8e-c2db-4d97-96c7-961178000240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3250/3250 [==============================] - 73s 22ms/step - loss: 214.9265 - mse: 214.9265 - val_loss: 6.2407 - val_mse: 6.2407\n",
      "Epoch 2/5\n",
      "3250/3250 [==============================] - 74s 23ms/step - loss: 5.3694 - mse: 5.3694 - val_loss: 4.7511 - val_mse: 4.7511\n",
      "Epoch 3/5\n",
      "3250/3250 [==============================] - 74s 23ms/step - loss: 4.1526 - mse: 4.1526 - val_loss: 4.2901 - val_mse: 4.2901\n",
      "Epoch 4/5\n",
      "3250/3250 [==============================] - 72s 22ms/step - loss: 3.4038 - mse: 3.4038 - val_loss: 4.2370 - val_mse: 4.2370\n",
      "Epoch 5/5\n",
      "3250/3250 [==============================] - 73s 23ms/step - loss: 2.7545 - mse: 2.7545 - val_loss: 3.8315 - val_mse: 3.8315\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_3_history = model_3.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "9141d0f2-64e6-4675-a761-efeb43b2eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25993, 1),\n",
       " array([[87.37346 ],\n",
       "        [87.28283 ],\n",
       "        [86.66328 ],\n",
       "        [91.63394 ],\n",
       "        [87.18992 ],\n",
       "        [89.89209 ],\n",
       "        [82.95627 ],\n",
       "        [85.261795],\n",
       "        [90.703476],\n",
       "        [89.23794 ]], dtype=float32))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs.shape, model_3_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "abcfd716-9e6f-48a7-8570-03ff0af21b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([87., 87., 87., 92., 87., 90., 83., 85., 91., 89.], dtype=float32)>"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round out predictions and reduce to 1-dimensional array\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "df9381db-a1fa-4179-885c-0433faae8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 1.507290424345016,\n",
       " 'MSE': 3.915015581117993,\n",
       " 'R-squared': 0.5799476515327149}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate LSTM model results\n",
    "model_3_results = calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4647f-7c77-4a23-92ec-ed4ad2ed975a",
   "metadata": {},
   "source": [
    "# model 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c36be-8994-4d1b-8e4d-e78cb43601a7",
   "metadata": {},
   "source": [
    "# Using Pretrained Embeddings (transfer learning for NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3f033978-9948-485e-9b93-cb174250374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157025  0.02485911  0.02878049 -0.01271502  0.03971542  0.08827761\n",
      "  0.02680984  0.05589836 -0.0106873  -0.00597294  0.00639323 -0.01819516\n",
      "  0.00030816  0.09105889  0.05874645 -0.03180627  0.01512473 -0.05162927\n",
      "  0.00991364 -0.06865346 -0.04209306  0.02678978  0.03011007  0.00321069\n",
      " -0.00337969 -0.04787355  0.02266721 -0.00985927 -0.04063614 -0.01292093\n",
      " -0.04666384  0.05630299 -0.03949255  0.00517685  0.02495828 -0.0701444\n",
      "  0.02871508  0.04947681 -0.00633978 -0.08960194  0.0280712  -0.00808365\n",
      " -0.01360601  0.05998649 -0.10361788 -0.05195374  0.00232957 -0.0233253\n",
      " -0.03758107  0.03327728], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8a78a1a5-ec11-4e8a-b42a-a75020dd7c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "     ------------------------------------ 100.6/100.6 kB 828.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\cbs\\anaconda3\\lib\\site-packages (from tensorflow_hub) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\cbs\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.23.5)\n",
      "Installing collected packages: tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d9ee5fd1-b679-48dc-8342-38a4310ad2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence has been encoded into a 512 dimension vector\n",
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "885088c1-5586-4edd-94dc-3af812880540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[], # shape of inputs coming to our model \n",
    "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
    "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
    "                                        name=\"USE\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "68323911-c979-44df-8555-8ca728f7471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_USE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create model using the Sequential API\n",
    "model_5 = tf.keras.Sequential([\n",
    "  sentence_encoder_layer,  # take in sentences and then encode them into an embedding\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(1 ,activation=\"linear\")  # Remove the activation function for regression\n",
    "], name=\"model_5_USE\")\n",
    "\n",
    "# Compile model\n",
    "model_5.compile(loss=tf.keras.losses.MeanSquaredError(),  # Use Mean Squared Error loss for regression\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mse\"])  # Use Mean Squared Error as the metric\n",
    "\n",
    "model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "6438b405-4fbc-42ab-a56b-0f8193b0aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "8ad02793-47d2-4d7e-aaaf-3461fdce71a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 383.4750 - mse: 383.4750 - val_loss: 32.1808 - val_mse: 32.1808\n",
      "Epoch 2/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 20.7007 - mse: 20.7007 - val_loss: 12.2451 - val_mse: 12.2451\n",
      "Epoch 3/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 9.2994 - mse: 9.2994 - val_loss: 7.4234 - val_mse: 7.4234\n",
      "Epoch 4/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 6.6122 - mse: 6.6122 - val_loss: 5.9386 - val_mse: 5.9386\n",
      "Epoch 5/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 5.6281 - mse: 5.6281 - val_loss: 5.4485 - val_mse: 5.4485\n",
      "Epoch 6/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 5.1744 - mse: 5.1744 - val_loss: 6.1867 - val_mse: 6.1867\n",
      "Epoch 7/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.9188 - mse: 4.9188 - val_loss: 4.8468 - val_mse: 4.8468\n",
      "Epoch 8/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.7717 - mse: 4.7717 - val_loss: 4.6469 - val_mse: 4.6469\n",
      "Epoch 9/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.6741 - mse: 4.6741 - val_loss: 5.1156 - val_mse: 5.1156\n",
      "Epoch 10/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.6164 - mse: 4.6164 - val_loss: 4.5908 - val_mse: 4.5908\n",
      "Epoch 11/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.5577 - mse: 4.5577 - val_loss: 4.4895 - val_mse: 4.4895\n",
      "Epoch 12/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.5307 - mse: 4.5307 - val_loss: 4.4351 - val_mse: 4.4351\n",
      "Epoch 13/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.5072 - mse: 4.5072 - val_loss: 4.4361 - val_mse: 4.4361\n",
      "Epoch 14/100\n",
      "3250/3250 [==============================] - 38s 12ms/step - loss: 4.4909 - mse: 4.4909 - val_loss: 4.5137 - val_mse: 4.5137\n",
      "Epoch 15/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.4612 - mse: 4.4612 - val_loss: 4.4236 - val_mse: 4.4236\n",
      "Epoch 16/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.4551 - mse: 4.4551 - val_loss: 4.4602 - val_mse: 4.4602\n",
      "Epoch 17/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.4489 - mse: 4.4489 - val_loss: 4.5626 - val_mse: 4.5626\n",
      "Epoch 18/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.4392 - mse: 4.4392 - val_loss: 4.5490 - val_mse: 4.5490\n",
      "Epoch 19/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.4310 - mse: 4.4310 - val_loss: 4.3725 - val_mse: 4.3725\n",
      "Epoch 20/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.4261 - mse: 4.4261 - val_loss: 4.4381 - val_mse: 4.4381\n",
      "Epoch 21/100\n",
      "3250/3250 [==============================] - 37s 12ms/step - loss: 4.4163 - mse: 4.4163 - val_loss: 4.3839 - val_mse: 4.3839\n",
      "Epoch 22/100\n",
      "3250/3250 [==============================] - 38s 12ms/step - loss: 4.4153 - mse: 4.4153 - val_loss: 4.3698 - val_mse: 4.3698\n",
      "Epoch 23/100\n",
      "3250/3250 [==============================] - 38s 12ms/step - loss: 4.4055 - mse: 4.4055 - val_loss: 4.3654 - val_mse: 4.3654\n",
      "Epoch 24/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.4075 - mse: 4.4075 - val_loss: 4.3829 - val_mse: 4.3829\n",
      "Epoch 25/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.3988 - mse: 4.3988 - val_loss: 4.6326 - val_mse: 4.6326\n",
      "Epoch 26/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.3964 - mse: 4.3964 - val_loss: 4.5635 - val_mse: 4.5635\n",
      "Epoch 27/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.3906 - mse: 4.3906 - val_loss: 4.3840 - val_mse: 4.3840\n",
      "Epoch 28/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.3888 - mse: 4.3888 - val_loss: 4.3375 - val_mse: 4.3375\n",
      "Epoch 29/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.3831 - mse: 4.3831 - val_loss: 4.3878 - val_mse: 4.3878\n",
      "Epoch 30/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.3786 - mse: 4.3786 - val_loss: 4.8656 - val_mse: 4.8656\n",
      "Epoch 31/100\n",
      "3250/3250 [==============================] - 36s 11ms/step - loss: 4.4012 - mse: 4.4012 - val_loss: 4.3181 - val_mse: 4.3181\n",
      "Epoch 32/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.3717 - mse: 4.3717 - val_loss: 4.3429 - val_mse: 4.3429\n",
      "Epoch 33/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.3788 - mse: 4.3788 - val_loss: 4.4932 - val_mse: 4.4932\n",
      "Epoch 34/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.3822 - mse: 4.3822 - val_loss: 4.3282 - val_mse: 4.3282\n",
      "Epoch 35/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.3644 - mse: 4.3644 - val_loss: 4.3242 - val_mse: 4.3242\n",
      "Epoch 36/100\n",
      "3250/3250 [==============================] - 37s 11ms/step - loss: 4.3654 - mse: 4.3654 - val_loss: 4.3542 - val_mse: 4.3542\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on top of pretrained embeddings\n",
    "model_5_history = model_5.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=100,\n",
    "                              validation_data=(val_sentences, val_labels),  callbacks=[early_stopping],\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "bee65c82-9bdb-46e1-89a9-94aa2cf3cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - 7s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[87.44683 ],\n",
       "       [84.11611 ],\n",
       "       [86.96642 ],\n",
       "       [93.322395],\n",
       "       [89.239174],\n",
       "       [87.98503 ],\n",
       "       [85.22346 ],\n",
       "       [87.420876],\n",
       "       [90.707375],\n",
       "       [90.30432 ]], dtype=float32)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with USE TF Hub model\n",
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5ccb6245-1ea0-4e4c-a00b-0f0bb52a0ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([87., 84., 87., 93., 89., 88., 85., 87., 91., 90.], dtype=float32)>"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to labels\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bf72fa54-47e4-41c0-9c01-3f6444a74cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 1.6393259723771785,\n",
       " 'MSE': 4.442465279113607,\n",
       " 'R-squared': 0.523356182162841}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 6 performance metrics\n",
    "model_5_results = calculate_results(val_labels, model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc522b-5dc5-43a7-8537-c7c8239d3322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml_2023",
   "language": "python",
   "name": "python_ml_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
